{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "MSa1f5Uengrz",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "-JiQyfWJYklI",
        "RoGjAbkUYoAp",
        "y-Ehk30pYrdP",
        "qYpmQ266Yuh3",
        "Seke61FWphqN",
        "b0JNsNcRphqO",
        "rFu4xreNphqO",
        "lssrdh5qphqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "YPEH6qLeZNRQ",
        "22aHeOlLveiV",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Hlsf0x5436Go",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rik8927/Capstone-project-Unsupervised-ML/blob/main/Netflix_Movies_And_TV_Shows_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -Netflix Movies And TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**Arghya Roy\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, you are required to do\n",
        "\n",
        "Exploratory Data Analysis\n",
        "\n",
        "Understanding what type content is available in different countries\n",
        "\n",
        "Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "\n",
        "Clustering similar content by matching text-based features"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import matplotlib.cm as cm\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "#for nlp\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "working_path = \"/content/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv\"\n",
        "df = pd.read_csv(working_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "AyekLSfXfrrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has 7787 row and 12 column."
      ],
      "metadata": {
        "id": "_DtDUsdbf0jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(duplicate_count)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False, yticklabels=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above it is clear that\n",
        "1. Director column has highest NaN values 30.7% data is missing\n",
        "2. cast column has 9% NaN values\n",
        "3. country , date_added , rating this columns also containing missing values\n",
        "4. Ploting the null values present in the dataset"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get a overall view of describe method."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "def DataInfoAll(df):\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    print(\"-\"*125)\n",
        "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
        "    summary = summary.reset_index()\n",
        "    summary['Name'] = summary['index']\n",
        "    summary = summary[['Name','dtypes']]\n",
        "    summary['Missing'] = df.isnull().sum().values\n",
        "    summary['Uniques'] = df.nunique().values\n",
        "    summary['First Value'] = df.iloc[0].values\n",
        "    summary['Second Value'] = df.iloc[1].values\n",
        "    return summary"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DataInfoAll(df)"
      ],
      "metadata": {
        "id": "7vEGAs2PhUWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking NAN values\n",
        "NaN_Checker = pd.DataFrame({\"No Of Total Values\": df.shape[0] , \"No of NaN values\": df.isnull().sum(),\n",
        "                    \"%age of NaN values\" : round((df.isnull().sum()/ df.shape[0])*100 , 2) })\n",
        "NaN_Checker.sort_values(\"No of NaN values\" , ascending = False)\n"
      ],
      "metadata": {
        "id": "8qBtM_Rthh1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_nan = df.isna()\n",
        "plot_nan.head(2)"
      ],
      "metadata": {
        "id": "ccLAxpgYhqeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above it is clear that\n",
        "\n",
        "Director column has highest NaN values 30.7% data is missing\n",
        "cast column has 9% NaN values\n",
        "country , date_added , rating this columns also containing missing values\n",
        "Ploting the null values present in the dataset"
      ],
      "metadata": {
        "id": "-j3dRjyWhmmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure( figsize = (10 , 5))\n",
        "sns.heatmap(plot_nan)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# null value distribution\n",
        "null_counts = df.isnull().sum()/len(df)\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.xticks(np.arange(len(null_counts)),null_counts.index,rotation='vertical')\n",
        "plt.ylabel('fraction of rows with missing data')\n",
        "plt.bar(np.arange(len(null_counts)),null_counts)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the NAN values we use this bar-plot graph."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Director and Cast column highest number of NAN values,so we have to drop it.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "df.drop(['director','cast'],axis=1,inplace=True)\n",
        "data_added_NaN = df[df['date_added'].isna()]\n",
        "data_added_NaN.head(2)\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_added_NaN.shape"
      ],
      "metadata": {
        "id": "FPtk_aGYnUnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are only 10 observation which are containing NAN values in data_added column."
      ],
      "metadata": {
        "id": "lpd57_xvn_aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.nunique()"
      ],
      "metadata": {
        "id": "Quvzw_k7nrFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['type'].unique()"
      ],
      "metadata": {
        "id": "l8Oe86tenuiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_movies=df[df.type =='TV Show']['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "yearly_shows=df[df.type =='Movie']['release_year'].value_counts().sort_index(ascending=False).head(15)\n",
        "total_content=df['release_year'].value_counts().sort_index(ascending=False).head(15)"
      ],
      "metadata": {
        "id": "JOxfRDxCn0ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_movies.head()"
      ],
      "metadata": {
        "id": "TFN1-9HRob36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.4)\n",
        "total_content.plot(figsize=(12, 6), linewidth=2.5, color='green',label=\"Total Content / year\")\n",
        "yearly_movies.plot(figsize=(12, 6), linewidth=2.5, color='maroon',label=\"Movies / year\",ms=3)\n",
        "yearly_shows.plot(figsize=(12, 6), linewidth=2.5, color='blue',label=\"TV Shows / year\")\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.legend()\n",
        "plt.title(\"Production Growth Yearly\", y=1.02, fontsize=22);"
      ],
      "metadata": {
        "id": "i69lPT14ofD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the growth of tv shows and movies year wise."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, highest spike in tv shows and movies between 2016 to 2018."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "#subsetting df\n",
        "df_wordcloud = df['title']\n",
        "text = \" \".join(word for word in df_wordcloud)\n",
        "# Create stopword list:\n",
        "stopwords = set(STOPWORDS)\n",
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out which type of word appeared most."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems words like 'Love','Man','World','Story','Christmas' are common in the title.\n",
        "Asonished that 'Christmas' occur maximum times,maybe the reason behind that maximum movie was released on the month of december.But i cant emphasize thats point because I dont have any inormation about release month data,that's why i am unable to check my hypothesis."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "country_list=[]\n",
        "tv_show=[]\n",
        "movies=[]\n",
        "for i in range(0,len(df)):\n",
        "  if isinstance(df['country'].iloc[i] , str):\n",
        "    split=df['country'].iloc[i].split(',')\n",
        "    for k in split:\n",
        "      country_list.append(k.strip())\n",
        "      if df['type'].iloc[i]=='TV Show':\n",
        "        tv_show.append(k.strip())\n",
        "      if df['type'].iloc[i]== 'Movie':\n",
        "        movies.append(k.strip())\n",
        "production_country=list(set([(i,country_list.count(i),tv_show.count(i),movies.count(i)) for i in country_list]))\n",
        "production_country[:5]"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country_df= pd.DataFrame(production_country,columns=['country','Productions','TV-Shows','Movies'])\n",
        "country_df=country_df.sort_values('Productions',ascending=False)\n",
        "country_df=country_df.reset_index()\n",
        "country_df=country_df.drop('index',axis=1)\n",
        "#Top 5 countries\n",
        "top_countries=country_df.head()\n",
        "top_countries"
      ],
      "metadata": {
        "id": "eSZnP5MAp9n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries.T"
      ],
      "metadata": {
        "id": "WwqgN2whqDAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_countries.head(9).plot(x=\"country\", y=['Productions','TV-Shows','Movies'], kind=\"bar\")"
      ],
      "metadata": {
        "id": "Hikkryb1qGNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out in which region or in which country highest number of production happened."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, from the baove graph it is clear that,\n",
        "###1.USA\n",
        "2. India\n",
        "3.United Kingdom\n",
        "4.Canada\n",
        "5.France"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#Plotting pie chart on type feature\n",
        "plt.figure(figsize=(14, 7))\n",
        "labels=['TV Show', 'Movie']\n",
        "plt.pie(df['type'].value_counts().sort_values(),labels=labels,explode=[0.01,0.01],\n",
        "        autopct='%1.2f%%', startangle=90)\n",
        "plt.title('Type of Netflix Content')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out how much percent its content movie and TV-shows."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Netlfix content,TV shows has 30 percent content and  near about 70 perent content comes under movie."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "xGcgTxhtsFFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How long the average episode time of Netflix?\n",
        "*    55 minutes.\n",
        "\n",
        "*   How many episode are in a season of  Netflix?\n",
        "*   5 seasons\n"
      ],
      "metadata": {
        "id": "Fimiis7AsJKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_seasons_to_min(value):\n",
        "  \"\"\"\n",
        "  This function will calculate no of total mins as per season no.\n",
        "  Here our assumptions are\n",
        "    1. on average 5 episodes are there in a season.\n",
        "    2. each episode avg time is 55 mins.\n",
        "  \"\"\"\n",
        "  no_of_avg_episode = 5\n",
        "  if \"Seasons\" in value:\n",
        "    #containing more than 1 seasons\n",
        "    value = value.replace(\"Seasons\",'')\n",
        "    value = value.replace(\" \",\"\")\n",
        "    total_seasons = int(value)\n",
        "    each_season_mins = ( no_of_avg_episode * 55 )\n",
        "    total_mins = (total_seasons * each_season_mins)\n",
        "    return total_mins\n",
        "\n",
        "  elif \"Season\" in value:\n",
        "    # containing only 1 season\n",
        "    value = value.replace(\"Season\",'')\n",
        "    value = value.replace(\" \",\"\")\n",
        "    total_mins = (no_of_avg_episode * 55)\n",
        "    return total_mins\n"
      ],
      "metadata": {
        "id": "UWIAYeZ4sPna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the function\n",
        "convert_seasons_to_min(\"4 Seasons\")"
      ],
      "metadata": {
        "id": "gXhV3kKSsTHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 seasons=(45) or 20 episodes and each episode average time is 55 minutes. so, the total time(in minutes)=(5520) minute =1100 minutes."
      ],
      "metadata": {
        "id": "AlQAPALUsWbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def all_the_duration_in_minutes():\n",
        "  \"\"\"\n",
        "  This function will convert all the duration\n",
        "  whether it's in minutes or season format to minute\n",
        "  \"\"\"\n",
        "  # replaced all the min with null string\n",
        "  df['duration'] = df.duration.str.replace(\" min\" , \"\")\n",
        "  # this time_list will contain all the value\n",
        "  time_list =[]\n",
        "  for time in df.duration.values:\n",
        "    if \"Season\" in time:\n",
        "      #time is containing Season\n",
        "      # calling convert_seasons_to_min function to convert\n",
        "      # season to total min\n",
        "      time = convert_seasons_to_min(time)\n",
        "    else:\n",
        "      #replacing single space with \"\"\n",
        "      time = time.replace(\" \",\"\")\n",
        "    #appending time (it's not containing words like min or seasons)\n",
        "    time_list.append(time)\n",
        "\n",
        "  #converting all the time into integer format\n",
        "  time_list = [ int(Time) for Time in time_list]\n",
        "\n",
        "  #Assigning time_list to df.duration\n",
        "  df.duration = time_list"
      ],
      "metadata": {
        "id": "t4Q-AQMNrtdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "QogVp3d8rzPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duration.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "tBtZJJ4MsmM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_the_duration_in_minutes()"
      ],
      "metadata": {
        "id": "w2fW6dYBshca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "sns.set(style=\"darkgrid\")\n",
        "plt.figure(figsize = (8,5))\n",
        "sns.kdeplot(data = df.duration[df['type'] == 'Movie'] , shade=True)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the timings of the most content."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most content are 70 to 120 minutes."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "df['type'].value_counts()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "plt.figure(figsize = (12,5))\n",
        "sns.kdeplot(data = df.duration[df['type'] == 'TV Show'] , shade=True)"
      ],
      "metadata": {
        "id": "YwVEYg-btO2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the duration of the TV shows."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can find out the most content of the tv shows duration betweeen 1000 minute."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "df.listed_in.nunique()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.listed_in.isna().sum()"
      ],
      "metadata": {
        "id": "ofTNJANLuUs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.listed_in.value_counts().to_frame().T"
      ],
      "metadata": {
        "id": "OWZdl596uXwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = \", \".join(df['listed_in']).split(\", \")\n",
        "categories[:5]"
      ],
      "metadata": {
        "id": "U0dvr7IGufhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(categories)"
      ],
      "metadata": {
        "id": "6Uoc2j-pulZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(categories))"
      ],
      "metadata": {
        "id": "xQp5hqTLunqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a dictionary ( category_wise_count ) where for each category there will be a value which basically tells us how many times that particular category occured."
      ],
      "metadata": {
        "id": "Aq__sNoMuscz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_wise_count = {}\n",
        "for category in set(categories):\n",
        "  category_wise_count[category] = categories.count(category)\n",
        "category_wise_count"
      ],
      "metadata": {
        "id": "yCh-aKxMurmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_category_wise_count = sorted(category_wise_count.items(), key=lambda x: x[1])\n",
        "sorted_category_wise_count[:4]"
      ],
      "metadata": {
        "id": "oxELoFUau6qG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_category_wise_count[:5]"
      ],
      "metadata": {
        "id": "-mUENrsQvBQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_category_wise_count[-5:]"
      ],
      "metadata": {
        "id": "kROfSLDbvE29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_categories = sorted_category_wise_count[-10:]"
      ],
      "metadata": {
        "id": "pLc5dug1vIW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_categories"
      ],
      "metadata": {
        "id": "zdl7G0IuvLuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_category_name = []\n",
        "top_10_most_occurred_category_count = []\n",
        "for tup in top_10_most_occurred_categories:\n",
        "  top_10_most_occurred_category_name.append(tup[0])\n",
        "  top_10_most_occurred_category_count.append(tup[1])"
      ],
      "metadata": {
        "id": "bZWjxmo7vPUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_category_name"
      ],
      "metadata": {
        "id": "o1QaHer8vTQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_10_most_occurred_category_count"
      ],
      "metadata": {
        "id": "At6yle1RvV2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = (16,8))\n",
        "color=['lightpink', 'violet', 'green', 'blue', 'cyan' , \"lightblue\" ,'red', 'pink', 'yellow', 'orange']\n",
        "plt.barh(top_10_most_occurred_category_name , top_10_most_occurred_category_count ,\n",
        "        color= color)\n",
        "plt.title(\"Top 10 Most Popular Categories\",fontsize = 19)\n",
        "plt.xlabel(\"Count\", fontsize = 14 )\n",
        "plt.ylabel(\"Category Name\" , fontsize = 14 )\n",
        "plt.figure( figsize = (16,8))\n"
      ],
      "metadata": {
        "id": "cc2VeatxvZE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the popular categories."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, top 10 most popuar categories are\n",
        "1. International Movies\n",
        "2. Dramas\n",
        "3. Comedies\n",
        "4. International Tv Shows\n",
        "5. Documentaries\n",
        "6. Action & Adventure\n",
        "7.TV Dramas\n",
        "8. Independent Movies\n",
        "9. Children and family movies\n",
        "10. Romantic Movies"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "type(df.listed_in.iloc[0])"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.listed_in.iloc[0])"
      ],
      "metadata": {
        "id": "qUzW5Kk9v519"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df.listed_in.iloc[0]).split(\",\")"
      ],
      "metadata": {
        "id": "rQGLmh1av9Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len((df.listed_in.iloc[0]).split(\",\"))"
      ],
      "metadata": {
        "id": "KLwlgmQLwAYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_of_category = []\n",
        "for categories in df.listed_in.values:\n",
        "  len_categories = len(categories.split(\",\"))\n",
        "  no_of_category.append(len_categories)"
      ],
      "metadata": {
        "id": "l_Mx2-tvwDND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['no_of_category'] = no_of_category"
      ],
      "metadata": {
        "id": "srME7NRewGFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['listed_in' , 'no_of_category']].head()"
      ],
      "metadata": {
        "id": "V5r_b_wUwIvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.no_of_category.unique()"
      ],
      "metadata": {
        "id": "HlIWa2vEwMQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.no_of_category.value_counts()"
      ],
      "metadata": {
        "id": "RMeHzvYGwOlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "plt.figure(figsize = (8,5))\n",
        "plt.hist(df.no_of_category , bins=[1,2,3,4] , range = (1 ,4) , rwidth = 0.85, color ='lightblue')\n",
        "plt.xlabel(\"No of categories\")\n",
        "plt.ylabel(\"Count\")"
      ],
      "metadata": {
        "id": "93lbEMPX4Cgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To count the number of categories."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So form the baove graph we found differnt umber of categories."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "df.date_added.head(5)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['date_added_month'] = df['date_added']\n",
        "df[['date_added' , 'date_added_month']].head()"
      ],
      "metadata": {
        "id": "yko-jwBD4Rya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_df=df['date_added_month'].value_counts().reset_index()\n",
        "month_df.rename(columns={'index': 'Month_Name'}, inplace=True)\n",
        "month_df.rename(columns={'month': 'Count'}, inplace=True)\n",
        "ab = month_df.loc[0:11]\n",
        "ab"
      ],
      "metadata": {
        "id": "YUDv_0wb4bJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_names = ab.Month_Name.values\n",
        "month_wise_count = ab.date_added_month.values"
      ],
      "metadata": {
        "id": "h99X1cFC4keJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20 ,16 ))\n",
        "plt.bar(month_names , month_wise_count, width = 0.91)\n",
        "plt.title(\"Month wise content barplot\" , fontsize = 19)\n",
        "plt.xlabel(\"Month\" , fontsize = 15)\n",
        "plt.ylabel(\"Count\" , fontsize = 15)"
      ],
      "metadata": {
        "id": "M1epbsrG4pFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "df['type'].unique()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tv_show = df[df['type']== 'TV Show' ]\n",
        "df_tv_show.head(2)"
      ],
      "metadata": {
        "id": "WqW-1pmo57y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pointplot on top tv show ratings\n",
        "tv_ratings = df_tv_show.groupby(['rating'])['show_id'].count().reset_index(name = 'count').sort_values(by = 'count', ascending = False)\n",
        "fig_dims = (7,4)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('Top TV Show Ratings Based On Rating System',size='15')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6y9ZTc0X5_CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies = df[df['type'] == 'Movie' ]\n",
        "df_movies.head(2)"
      ],
      "metadata": {
        "id": "pb6ycOJG6V-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "tv_ratings = df_movies.groupby(['rating'])['show_id'].count().reset_index(name = 'count').sort_values(by = 'count', ascending = False)\n",
        "fig_dims = (10,6)\n",
        "fig, ax = plt.subplots(figsize=fig_dims)\n",
        "sns.pointplot(x='rating',y='count',data=tv_ratings)\n",
        "plt.title('Top Movie Ratings Based On Rating System',size='20')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df.corr(),annot=True)"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out the relation between the parameters-\n",
        "1 indicates a perfect positive correlation\n",
        "(-1) indicates a perfect negative correlation\n",
        "0 indicates no correlation."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "# pairplot with hue sex\n",
        "seaborn.pairplot(df)\n",
        "# to show\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the overall idea or relation about the data."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "so from the above pairplot,we can get a clear view of about the viewer interest on netflix."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "\n",
        "H0: There is no significant association between type and country.\n",
        "In both cases, the null hypothesis suggests that there is no relationship or association between the two categorical variables.\n",
        "\n",
        "Alternative Hypothesis (Ha):\n",
        "Ha: There is a significant association between type and country.\n",
        "The alternative hypothesis suggests that there is a significant relationship or association between the two categorical variable. chi-square test for independence to analyze the data and determine whether the p-value obtained from the test is less than the chosen significance level (e.g., α = 0.05). If the p-value is less than α, we can reject the null hypothesis, indicating that there is a significant association between type and country. If the p-value is greater than α, we would fail to reject the null hypothesis, suggesting no significant association."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "contingency_table = pd.crosstab(df['type'], df['country'])\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "print(f\"Chi-Square Test Statistic: {chi2}\")\n",
        "print(f\"P-value: {p}\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant relationship between type and country.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant relationship between type and country.\")"
      ],
      "metadata": {
        "id": "W5u5ex9aXXzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "chi-square test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chi-square test helps to determine whether there is a statistically significant association or relationship between two categorical variables—in this case,type and country. This can help you understand if passenger type influences the likelihood of making referrals."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "\n",
        "Null Hypothesis: The coefficient (β) for a specific predictor variable in the logistic regression model is equal to zero.\n",
        "Symbolically: H0: β = 0\n",
        "Interpretation: This null hypothesis assumes that the predictor has no effect on the log-odds of the binary outcome variable. In other words, the predictor is not associated with the outcome.\n",
        "Alternative Hypothesis (Ha):\n",
        "\n",
        "Alternative Hypothesis: The coefficient (β) for a specific predictor variable in the logistic regression model is not equal to zero.\n",
        "Symbolically: Ha: β ≠ 0\n",
        "Interpretation: The alternative hypothesis suggests that the predictor has a statistically significant effect on the log-odds of the binary outcome variable. In other words, the predictor is associated with the outcome."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "contingency_table = pd.crosstab(df['type'], df['country'])\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table, lambda_=\"log-likelihood\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant relationship between type and country.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant relationship between type and country.\")"
      ],
      "metadata": {
        "id": "8fyew2G0YtwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "G-test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of Netflix movie clustering, we may want to use the G-test to determine if there is a statistically significant association between the type and the country. If the relation between type and the country is suitable then the customer satisfiction is good.so,we can set our marketing strategy according to that."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "\n",
        "Null Hypothesis: The coefficient (β) for a specific predictor variable in the logistic regression model is equal to zero.\n",
        "Symbolically: H0: β = 0\n",
        "Interpretation: This null hypothesis assumes that the predictor has no effect on the log-odds of the binary outcome variable. In other words, the predictor is not associated with the outcome.\n",
        "Alternative Hypothesis (Ha):\n",
        "\n",
        "Alternative Hypothesis: The coefficient (β) for a specific predictor variable in the logistic regression model is not equal to zero.\n",
        "Symbolically: Ha: β ≠ 0\n",
        "Interpretation: The alternative hypothesis suggests that the predictor has a statistically significant effect on the log-odds of the binary outcome variable. In other words, the predictor is associated with the outcome."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.api as sm\n",
        "df_cleaned = df.dropna()\n",
        "df_cleaned['no_of_category'] = df_cleaned['no_of_category'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "df_cleaned['release_year'] = df_cleaned['release_year'].apply(lambda x: 1 if x > 0.5 else 0)\n",
        "X = df_cleaned[['no_of_category', 'release_year']]\n",
        "X = pd.get_dummies(X, columns=['release_year'], drop_first=True)  # Dummy encode passenger type\n",
        "y = df_cleaned['no_of_category']\n",
        "X = sm.add_constant(X)  # Add a constant (intercept) term\n",
        "model = sm.Logit(y, X)\n",
        "results = model.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant relationship between no_of_category and release_year.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant relationship between no_of_category and release_year.\")"
      ],
      "metadata": {
        "id": "9Uk2Abg7YweS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression provides easily interpretable results. You can interpret the coefficients of predictor variables as the log-odds of the outcome, allowing you to understand how each predictor influences the likelihood of a referral. This interpretability can be valuable for making business decisions."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "sns.boxplot(df.release_year)"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen,before 2014 the production growth for Movies & Tv Shows were very less ,that's why it's showing the values(release_year less than 2009) as outliers."
      ],
      "metadata": {
        "id": "lHHs7I9A7JjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Type of data in release_year\n",
        "type(df.release_year[0])"
      ],
      "metadata": {
        "id": "b-Zxlrwb7MWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "release_year_Q1 = df.release_year.quantile(0.25)\n",
        "release_year_Q3 = df.release_year.quantile(0.75)\n",
        "release_year_IQR = release_year_Q3 - release_year_Q1\n",
        "print(f'release_year_Q1 = {release_year_Q1}\\nrelease_year_Q3 = {release_year_Q3}\\nrelease_year_IQR = {release_year_IQR}')"
      ],
      "metadata": {
        "id": "u_xx0fiQ7QRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so, we dont have any release_year which is greater than 2018."
      ],
      "metadata": {
        "id": "XbBGayLD7bge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "release_year_outliers = df[(df.release_year < (release_year_Q1 - 1.5 * release_year_IQR)) |\n",
        "                           ( df.release_year > (release_year_Q3 + 1.5 * release_year_IQR)) ]\n",
        "release_year_outliers"
      ],
      "metadata": {
        "id": "s5XQ5e-h7fz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 percentile value is 2009\n",
        "df[\"release_year\"] = np.where(df[\"release_year\"] <2009, df.release_year.mean(),df['release_year'])"
      ],
      "metadata": {
        "id": "Dpl7rWzp7shG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(df.release_year)"
      ],
      "metadata": {
        "id": "Zyhz45qs7zIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Datatype of release_year = \",type(df.release_year.iloc[0]))\n",
        "df.release_year = df.release_year.astype(\"int64\")\n",
        "print(f\"Datatype of release_year = \",type(df.release_year.iloc[0]))"
      ],
      "metadata": {
        "id": "zokitzzI748H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above code it is clear that:\n",
        "###we first use quantile method to understand the distribution of data.\n",
        "###Then we detect the outliers of the release year.\n",
        "###Then we use the mean term to reduce the outliers.\n",
        "###After that we use box plot graph to detect is the re any outliers present or not."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "df.columns"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.description.iloc[0]"
      ],
      "metadata": {
        "id": "NQJF7oeInZ3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "First_des = df.description.iloc[0]\n",
        "First_des"
      ],
      "metadata": {
        "id": "7yHIzLaOmSiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "SCIW_bNqnmw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "vk9kUut-nr7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "def remove_stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    #Method 1\n",
        "    text1 = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text1)"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space,\n",
        "    # which in effect deletes the punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(remove_punctuation)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jfbxlsqsolHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "for i in sw:\n",
        "  print(i , end=',  ')"
      ],
      "metadata": {
        "id": "EDz_25QxouCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of stopwords in english : \", len(sw))"
      ],
      "metadata": {
        "id": "QBE61BYnraHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    #Method 1\n",
        "    text1 = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text1)\n"
      ],
      "metadata": {
        "id": "HBRQVh3Dihaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply( remove_stopwords )\n",
        "df.head()"
      ],
      "metadata": {
        "id": "jJzOQ6umrsbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all the values of description are punctuation free and stopword free."
      ],
      "metadata": {
        "id": "W3LkXo-srzMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Create a count vectorizer object\n",
        "count_vectorizer = CountVectorizer()\n",
        "# fit the count vectorizer using the text data\n",
        "count_vectorizer.fit(df['description'])\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = count_vectorizer.vocabulary_.items()\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary"
      ],
      "metadata": {
        "id": "QMfcpsVwsVF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [ ]\n",
        "count_of_vocab = []\n",
        "for key , value in dictionary:\n",
        "  vocab.append( key )\n",
        "  count_of_vocab.append( value )"
      ],
      "metadata": {
        "id": "yVL03UqAlsGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the count in panadas dataframe with vocab as index\n",
        "vocab_before_stemming = pd.DataFrame({\"Word\": vocab ,\n",
        "                                      \"count\" :count_of_vocab})\n",
        "# Sort the dataframe\n",
        "vocab_before_stemming = vocab_before_stemming.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "ny11BwKcnvE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before_stemming.head(4)"
      ],
      "metadata": {
        "id": "G_QlxzP1l0vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before_stemming.head(20).T"
      ],
      "metadata": {
        "id": "Bft1LkSul4St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_before_stemming.tail(4)"
      ],
      "metadata": {
        "id": "9rpxH-U9l7-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vacab = vocab_before_stemming.head(15)\n",
        "top15_most_occurred_words = top15_most_ocurred_vacab.Word.values\n",
        "top15_most_occurred_words"
      ],
      "metadata": {
        "id": "golai6EZl9sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_count = top15_most_ocurred_vacab['count'].values\n",
        "top15_most_occurred_words_count"
      ],
      "metadata": {
        "id": "0R02DJ8XmFzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 16,6 ))\n",
        "plt.xlim(19550, 19600)\n",
        "plt.barh(top15_most_occurred_words , top15_most_occurred_words_count )"
      ],
      "metadata": {
        "id": "KgxcyZZQqXkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object of stemming function\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "AwdqCq9zq5QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Apply_stemming(text):\n",
        "    '''a function which stems each word in the given text'''\n",
        "    text = [stemmer.stem(word) for word in text.split()]\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "z-HJuvqyq9Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming for description\n",
        "df['description'] = df['description'].apply( Apply_stemming )\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0EcwB3bhrAgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the object of tfid vectorizer\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfid_vectorizer.fit(df['description'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfid_vectorizer.vocabulary_.items()"
      ],
      "metadata": {
        "id": "ckvRe3CFrGP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store the vocab and counts\n",
        "vocab = []\n",
        "count_of_vocab = []\n",
        "# Iterate through each vocab and count append the value to designated lists\n",
        "for key, value in dictionary:\n",
        "    vocab.append(key)\n",
        "    count_of_vocab.append(value)"
      ],
      "metadata": {
        "id": "9LUahpHrrKgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the count in panadas dataframe with vocab as index\n",
        "vocab_after_stemming = pd.DataFrame({\"Word\": vocab ,\n",
        "                                      \"count\" :count_of_vocab})\n",
        "# Sort the dataframe\n",
        "vocab_after_stemming = vocab_after_stemming.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "94GRQz3FrOWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vocab = vocab_after_stemming.head(15)"
      ],
      "metadata": {
        "id": "v0yiI16HrSDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words = top15_most_ocurred_vocab.Word.values\n",
        "top15_most_occurred_words"
      ],
      "metadata": {
        "id": "I9trSOeQrWrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_count = top15_most_ocurred_vocab['count'].values\n",
        "top15_most_occurred_words_count"
      ],
      "metadata": {
        "id": "puKrArB3raeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 16,6 ))\n",
        "plt.xlim(14225, 14241)\n",
        "plt.barh(top15_most_occurred_words , top15_most_occurred_words_count )"
      ],
      "metadata": {
        "id": "rNVOVDtjrgCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Length(description)'] = df['description'].apply(lambda x: len(x))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "no-bxM9IrlAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.description.iloc[0]"
      ],
      "metadata": {
        "id": "yZSAuxM3rt14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.description.iloc[0])"
      ],
      "metadata": {
        "id": "9DtzKSs8rysb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a count vectorizer object\n",
        "count_vectorizer = CountVectorizer()\n",
        "# fit the count vectorizer using the text data\n",
        "count_vectorizer.fit(df['listed_in'])\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = count_vectorizer.vocabulary_.items()"
      ],
      "metadata": {
        "id": "XVYySt5cu5oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary"
      ],
      "metadata": {
        "id": "a8aSrFz3u-ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [ ]\n",
        "count_of_vocab = []\n",
        "for key , value in dictionary:\n",
        "  vocab.append( key )\n",
        "  count_of_vocab.append( value )"
      ],
      "metadata": {
        "id": "SFpIv6RvY9dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed_in_vocab_before_stem = pd.DataFrame({\"Word\": vocab , \"count\" :count_of_vocab})\n",
        "\n",
        "listed_in_vocab_before_stem = listed_in_vocab_before_stem.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "3X7wd4nYvN3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed_in_vocab_before_stem.head()"
      ],
      "metadata": {
        "id": "hwsSHCLnvShw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listed_in_vocab_before_stem.tail()"
      ],
      "metadata": {
        "id": "ukNZGabdvYmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vocab_listed_in = listed_in_vocab_before_stem.head(15)"
      ],
      "metadata": {
        "id": "tQNLzbqavh1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_words_listed_in = top15_most_ocurred_vocab_listed_in.Word.values\n",
        "top15_most_ocurred_words_listed_in"
      ],
      "metadata": {
        "id": "daJg95ESvlhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_in_listed_in_count = top15_most_ocurred_vocab_listed_in['count'].values\n",
        "top15_most_occurred_words_in_listed_in_count"
      ],
      "metadata": {
        "id": "isZlMLErvp-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 10,6 ))\n",
        "plt.xlim(25, 42 )\n",
        "plt.barh(top15_most_ocurred_words_listed_in , top15_most_occurred_words_in_listed_in_count )"
      ],
      "metadata": {
        "id": "fHO7sw0RvucV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming for description\n",
        "df['listed_in'] = df['listed_in'].apply( Apply_stemming )\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "YkVoLcp0vzfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the object of tfid vectorizer\n",
        "tfid_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer using the text data\n",
        "tfid_vectorizer.fit(df['listed_in'])\n",
        "\n",
        "# Collect the vocabulary items used in the vectorizer\n",
        "dictionary = tfid_vectorizer.vocabulary_.items()"
      ],
      "metadata": {
        "id": "WW9m4qcZv7q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary"
      ],
      "metadata": {
        "id": "J-c0CCLpv_rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists to store the vocab and counts\n",
        "vocab = []\n",
        "count_of_vocab = []\n",
        "# Iterate through each vocab and count append the value to designated lists\n",
        "for key, value in dictionary:\n",
        "    vocab.append(key)\n",
        "    count_of_vocab.append(value)"
      ],
      "metadata": {
        "id": "LffT1XCkwEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_after_stemming_listed_in = pd.DataFrame({\"Word\": vocab , \"count\" :count_of_vocab})\n",
        "# Sort the dataframe by count\n",
        "vocab_after_stemming_listed_in = vocab_after_stemming_listed_in.sort_values(\"count\" ,ascending=False)"
      ],
      "metadata": {
        "id": "1yBCoVAzwJ30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vocab_lised_in_after_stem = vocab_after_stemming_listed_in.head(15)"
      ],
      "metadata": {
        "id": "GqsBLhp1wOgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_ocurred_vocab_lised_in_after_stem_word = top15_most_ocurred_vocab_lised_in_after_stem.Word.values\n",
        "top15_most_ocurred_vocab_lised_in_after_stem_word"
      ],
      "metadata": {
        "id": "kQYeY2RbwShx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top15_most_occurred_words_listed_in_count = top15_most_ocurred_vocab_lised_in_after_stem['count'].values\n",
        "top15_most_occurred_words_listed_in_count"
      ],
      "metadata": {
        "id": "jkuTOgFawYWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure( figsize = ( 10,6 ))\n",
        "plt.xlim(25, 40 )\n",
        "plt.barh(top15_most_ocurred_vocab_lised_in_after_stem_word , top15_most_occurred_words_listed_in_count )"
      ],
      "metadata": {
        "id": "TgrwM0OzwcSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Length(listed-in)'] = df['listed_in'].apply(lambda x: len(x))\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "6Jrh3CYxwlJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "BAsdr4sjwpxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['description', 'Length(description)', 'listed_in' ,'Length(listed-in)' ]].head(3)"
      ],
      "metadata": {
        "id": "HIq3pSRZwu3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_features_rec = df[['no_of_category' ,'Length(description)','Length(listed-in)']]\n",
        "stdscaler = preprocessing.StandardScaler()"
      ],
      "metadata": {
        "id": "KGcERm1AySfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_features_rec.describe()"
      ],
      "metadata": {
        "id": "QK-_jVrCyWbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_rescale=stdscaler.fit_transform(X_features_rec)\n",
        "X=X_rescale\n",
        "silhouette_score_ = [  ]\n",
        "range_n_clusters = [i for i in range(2,16)]"
      ],
      "metadata": {
        "id": "TlCNWG_3yfhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEQ5rNubyPYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "for n_clusters in range_n_clusters:\n",
        "    clusterer = KMeans(n_clusters=n_clusters)\n",
        "    preds = clusterer.fit_predict(X)\n",
        "    centers = clusterer.cluster_centers_\n",
        "\n",
        "    score = silhouette_score(X, preds)\n",
        "    silhouette_score_.append([int(n_clusters) , round(score , 3)])\n",
        "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))"
      ],
      "metadata": {
        "id": "WJUoN6vHKSU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = pd.DataFrame(silhouette_score_ , columns = [\"n clusters\" , \"silhouette score\"])\n",
        "temp = temp.sort_values( \"silhouette score\" , ascending = False )\n",
        "temp.head(14)"
      ],
      "metadata": {
        "id": "VYY5_yj2KVRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The value of the silhouette coefﬁcient is between [-1, 1]. A score of 1 denotes the best meaning that the data point i is very compact within the cluster to which it belongs and far away from the other clusters. The worst value is -1. Values near 0 denote overlapping clusters"
      ],
      "metadata": {
        "id": "qHd3mRrIKoOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_n_clusters = [i for i in range(2,16)]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='o', alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zF9NO_XfK0xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Elbow Method\n",
        "sum_of_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
        "    km = km.fit(X)\n",
        "    sum_of_sq_dist[k] = km.inertia_\n",
        "\n",
        "#Plot the graph for the sum of square distance values and Number of Clusters\n",
        "sns.pointplot(x = list(sum_of_sq_dist.keys()), y = list(sum_of_sq_dist.values()))\n",
        "plt.xlabel('Number of Clusters(k)')\n",
        "plt.ylabel('Sum of Square Distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ipKV7QPBLIc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 3 )\n",
        "kmeans.fit(X)\n",
        "y_kmeans= kmeans.predict(X)"
      ],
      "metadata": {
        "id": "Y88U6KYaLXR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10 , 6))\n",
        "plt.title('description and listed_in')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='spring')\n",
        "\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5)"
      ],
      "metadata": {
        "id": "EK_6h2tlLZht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means in terms of how well samples are clustered with other samples that are similar to each other. The Silhouette score is calculated for each sample of different clusters. To calculate the Silhouette score for each observation/data point, the following distances need to be found out for each observations belonging to all the clusters:\n",
        "\n",
        "Mean distance between the observation and all other data points in the same cluster. This distance can also be called a mean intra-cluster distance. The mean distance is denoted by a.\n",
        "\n",
        "* Mean distance between the observation and all other data points of the next nearest cluster. This distance can also be called a mean nearest-cluster distance. The mean distance is denoted by b.\n",
        "\n",
        "The Silhouette Coefficient for a sample is s=(b-a)/max(a,b)\n",
        "###The \"elbow method\" is a popular technique used to determine the optimal number of clusters (k) in a clustering algorithm, such as K-means. It is called the \"elbow method\" because, when you plot the within-cluster sum of squares (WCSS) or distortion as a function of the number of clusters, the plot often looks like an \"elbow\" with a noticeable bend or inflection point. This bend in the plot can be used as a heuristic for selecting an appropriate number of clusters."
      ],
      "metadata": {
        "id": "rTORygmP-Qz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#DBSCAN\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "y_pred = DBSCAN(eps=0.5, min_samples=15).fit_predict(X)\n",
        "plt.figure(figsize=( 8 , 5 ))\n",
        "plt.scatter(X[:,1], X[:,2], c=y_pred)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The black colour dots(*) are noise."
      ],
      "metadata": {
        "id": "Fwp__JRGxBuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's import sch\n",
        "import scipy.cluster.hierarchy as sch\n",
        "plt.figure(figsize=(13,8))\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'single'))\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show() # find largest vertical distance we can make without crossing any other horizontal line"
      ],
      "metadata": {
        "id": "a9q4-orzxH88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's  import AgglomerativeClustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters = 2, affinity = 'euclidean', linkage = 'ward')\n",
        "y_hc = hc.fit_predict(X)\n",
        "# Visualizing the clusters (three dimensions only)\n",
        "plt.figure(figsize=(13,8))\n",
        "plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = '1')\n",
        "plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = '2')\n",
        "\n",
        "\n",
        "plt.title('Clusters of content')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AZV8boOqxOEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that is suitable for a variety of unsupervised machine learning tasks, including those involving Netflix user data. While the choice of clustering algorithm depends on the specific nature of the data and the goals of the analysis, DBSCAN can be a valuable option for Netflix unsupervised ML for several reasons:\n",
        "\n",
        "Handling Complex Cluster Shapes: DBSCAN is effective at identifying clusters with irregular shapes and varying densities. In the context of Netflix user data, where user behaviors and preferences can be highly diverse and may not follow a simple pattern, DBSCAN's ability to discover non-convex clusters can be advantageous.\n",
        "\n",
        "Automatic Determination of Cluster Count: Unlike some other clustering algorithms (e.g., K-means), DBSCAN does not require you to specify the number of clusters in advance. It dynamically determines the number of clusters based on the density and distribution of data points, making it suitable when you don't have prior knowledge of the expected number of user segments.\n",
        "\n",
        "Noise Detection: DBSCAN is capable of identifying and labeling noisy data points as outliers or noise. In Netflix user data, where some users may exhibit unusual or outlier behaviors, this feature can help in filtering out such noise and focusing on meaningful clusters.\n",
        "\n",
        "Adaptive Distance Metric: DBSCAN uses a distance-based metric to define neighborhoods, and it adapts to the local density of data. This means that DBSCAN can handle datasets with varying data densities effectively, which is particularly relevant for user behavior data on a platform like Netflix\n",
        "###Agglomerative hierarchical clustering is one of several clustering algorithms that can be considered for unsupervised machine learning tasks, including those involving Netflix user data. The choice of clustering algorithm, including agglomerative clustering, depends on the nature of the data and the goals of your analysis. Here are some reasons why you might consider using agglomerative clustering for Netflix unsupervised ML:\n",
        "\n",
        "Hierarchical Structure: Agglomerative clustering creates a hierarchical representation of clusters, known as a dendrogram. This hierarchical structure can be useful for gaining a deep understanding of how data points are grouped and organized. It allows you to explore clusters at different levels of granularity, which can be valuable for analyzing user behaviors and preferences on a platform like Netflix.\n",
        "\n",
        "Variable Cluster Sizes: Agglomerative clustering does not require you to specify the number of clusters (k) in advance, unlike K-means or DBSCAN. This can be advantageous when you don't have prior knowledge about the optimal number of clusters within your data. Agglomerative clustering will automatically determine the number of clusters based on the data.\n",
        "\n",
        "Interpretability: Agglomerative clustering provides a straightforward and interpretable way to analyze clusters. You can visually examine the dendrogram to see how clusters are merged, and you can choose the number of clusters that align with your understanding of the data.\n",
        "\n",
        "Visualizations: The hierarchical nature of agglomerative clustering lends itself well to visualizations. You can create dendrograms that illustrate the merging of clusters over different levels, making it easier to identify patterns and relationships in your data.\n",
        "\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBScan model."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice between K-means and DBSCAN for an unsupervised machine learning task, such as analyzing Netflix user data, depends on the specific characteristics of your dataset, your goals, and the nature of the problem you are trying to solve. Both algorithms have their strengths and weaknesses, so it's important to consider several factors when making your decision:\n",
        "\n",
        "Cluster Shape:\n",
        "\n",
        "K-Means: Assumes that clusters are spherical, equally sized, and have similar densities. It may not perform well on datasets with irregularly shaped or non-convex clusters.\n",
        "DBSCAN: Can discover clusters of arbitrary shapes, making it suitable for datasets with complex and non-uniform cluster structures.\n",
        "Scalability:\n",
        "\n",
        "K-Means: Scales well with large datasets and is computationally efficient.\n",
        "DBSCAN: Can become less efficient as the dataset size grows, particularly with the default implementation. However, there are optimized versions of DBSCAN for large datasets.\n",
        "Cluster Density:\n",
        "\n",
        "K-Means: Performs best when clusters have roughly equal densities.\n",
        "DBSCAN: Is robust to varying cluster densities. It can identify both dense and sparse regions effectively.\n",
        "Noise Handling:\n",
        "\n",
        "K-Means: Assigns every data point to a cluster, which may lead to the creation of noisy clusters.\n",
        "DBSCAN: Can identify and label noisy data points as outliers, allowing for a more accurate representation of the data.\n",
        "Number of Clusters (k):\n",
        "\n",
        "K-Means: Requires specifying the number of clusters (k) in advance. You need to have some prior knowledge or use techniques like the elbow method to estimate an appropriate k.\n",
        "DBSCAN: Does not require specifying the number of clusters in advance. It automatically discovers clusters based on data density.\n",
        "Interpretability:\n",
        "\n",
        "K-Means: Provides straightforward cluster assignments, which can be more interpretable.\n",
        "DBSCAN: May assign data points to a \"noise\" cluster, which can complicate interpretation but also helps identify outliers.\n",
        "Parameter Tuning:\n",
        "\n",
        "K-Means: Typically involves tuning the number of clusters (k) and initialization methods.\n",
        "DBSCAN: Requires tuning parameters such as the minimum number of points in a cluster (minPts) and the maximum distance (epsilon) to define neighborhood.\n",
        "For Netflix user data, which can be complex and have varying user behaviors, DBSCAN might be a good starting point due to its ability to handle irregularly shaped clusters and varying cluster densities. However, you should also consider preprocessing steps, feature engineering, and data exploration to ensure that the chosen clustering algorithm aligns with your objectives and the nature of the user data."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "Director and cast contains a large number of null values so we will drop these 2 columns .\n",
        "In this dataset there are two types of contents where 30.86% includes TV shows and the\n",
        "remaining 69.14% carries Movies. 3. We have reached a conclusion from our analysis from the content added over years that Netflix is focusing movies and TV shows (Fom 2016 data we get to know that Movies is increased by 80% and TV shows is increased by 73% compare) 4. From the dataset insights we can conclude that the most number of TV Shows released in 2017 and for Movies it is 2020 5. On Netflix USA has the largest number of contents. And most of the countries preferred to produce movies more than TV shows. 6. Most of the movies are belonging to 3 categories 7. TOP 3 content categories are International movies , dramas , comedies. 8. In text analysis (NLP) I used stop words, removed punctuations , stemming & TF-IDF vectorizer and other functions of NLP. 9. Applied different clustering models like Kmeans, hierarchical, Agglomerative clustering, DBSCAN on data we got the best cluster arrangements. 10.By applying different clustering algorithms to our dataset .we get the optimal number of cluster is equal to 3\n",
        "\n",
        "Futue Work\n",
        "From this clustering analysis we can create Netflix movies and tv shows recommendation system & also we can use topic modeling."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}